# Guia Pr√°tico de Treino e Simula√ß√£o: World Model (Fase 1)

**Autor:** Manus AI

**Data:** 2 de Fevereiro de 2026

## Introdu√ß√£o

Esta √© a continua√ß√£o do nosso guia de implementa√ß√£o. Aqui, vamos focar-nos no **como**: como treinar e simular o seu sistema de Fase 1. A escolha mais inteligente para a simula√ß√£o, especialmente sem c√¢mara, √© o **MuJoCo (Multi-Joint dynamics with Contact)**.

**Porqu√™ o MuJoCo?**

- **Velocidade e Precis√£o:** √â um dos simuladores de f√≠sica mais r√°pidos e precisos dispon√≠veis, otimizado para din√¢micas de contacto, o que √© essencial para manipula√ß√£o rob√≥tica.
- **Padr√£o da Ind√∫stria:** √â amplamente utilizado em investiga√ß√£o de rob√≥tica e reinforcement learning (DeepMind, OpenAI, etc.).
- **Excelentes Bindings Python:** Integra-se perfeitamente com o ecossistema Python, especialmente com a biblioteca `gymnasium`, que fornece uma interface padr√£o para ambientes de RL.

Vamos estruturar este guia em tr√™s partes: o setup do ambiente, o pipeline de treino detalhado com c√≥digo, e como executar o agente final.

## 1. Setup do Ambiente de Simula√ß√£o (MuJoCo)

O cora√ß√£o de uma simula√ß√£o MuJoCo √© um ficheiro XML que define o mundo: o rob√¥, os objetos, as suas propriedades f√≠sicas e as suas rela√ß√µes.

### 1.1. O Ficheiro XML do Mundo (`robot_world.xml`)

Crie um ficheiro chamado `robot_world.xml`. Este ficheiro ir√° descrever um bra√ßo rob√≥tico simples de 3 juntas, um objeto c√∫bico que ele pode manipular, e um local-alvo.

```xml
<mujoco model="3_joint_arm">
  <compiler angle="degree" coordinate="local" inertiafromgeom="true"/>
  <option integrator="RK4" timestep="0.01"/>

  <default>
    <joint armature="1" damping="1" limited="true"/>
    <geom conaffinity="0" condim="3" density="5.0" friction="1 0.5 0.5" margin="0.01" rgba="0.8 0.6 0.4 1"/>
  </default>

  <worldbody>
    <light cutoff="100" diffuse="1 1 1" dir="-0 0 -1.3" directional="true" exponent="1" pos="0 0 1.3" specular=".1 .1 .1"/>
    <geom conaffinity="1" condim="3" name="floor" pos="0 0 0" rgba="0.8 0.9 0.8 1" size="1 1 0.1" type="plane"/>
    
    <!-- Bra√ßo Rob√≥tico -->
    <body name="base" pos="0 0 0.1">
      <joint type="free"/>
      <geom name="base_geom" pos="0 0 0" size="0.05 0.05 0.05" type="box"/>
      <body name="link1" pos="0 0 0.05">
        <joint axis="0 0 1" name="joint1" pos="0 0 0" range="-180 180" type="hinge"/>
        <geom fromto="0 0 0 0.3 0 0" name="link1_geom" size="0.04" type="capsule"/>
        <body name="link2" pos="0.3 0 0">
          <joint axis="0 1 0" name="joint2" pos="0 0 0" range="-120 120" type="hinge"/>
          <geom fromto="0 0 0 0.3 0 0" name="link2_geom" size="0.04" type="capsule"/>
          <body name="link3" pos="0.3 0 0">
            <joint axis="0 1 0" name="joint3" pos="0 0 0" range="-120 120" type="hinge"/>
            <geom fromto="0 0 0 0.2 0 0" name="link3_geom" size="0.04" type="capsule"/>
            <site name="end_effector" pos="0.2 0 0" size="0.01"/>
          </body>
        </body>
      </body>
    </body>

    <!-- Objeto a ser manipulado -->
    <body name="object" pos="0.5 0.1 0.05">
        <joint type="free" damping="0.01"/>
        <geom name="object_geom" size="0.05 0.05 0.05" type="box" rgba="0.2 0.2 1.0 1"/>
    </body>

    <!-- Alvo -->
    <site name="target" pos="-0.5 -0.5 0.1" size="0.05" rgba="1 0.2 0.2 0.5" type="sphere"/>

  </worldbody>

  <actuator>
      <motor joint="joint1" ctrllimited="true" ctrlrange="-1.0 1.0" gear="150.0"/>
      <motor joint="joint2" ctrllimited="true" ctrlrange="-1.0 1.0" gear="150.0"/>
      <motor joint="joint3" ctrllimited="true" ctrlrange="-1.0 1.0" gear="150.0"/>
  </actuator>
</mujoco>
```

### 1.2. Interagindo com o Ambiente em Python

Agora, vamos usar o `gymnasium` para carregar e controlar este mundo. O Gymnasium fornece uma interface `Env` padr√£o com m√©todos como `step()` e `reset()`.

**Instala√ß√£o:**
```bash
sudo pip3 install gymnasium gymnasium[mujoco]
```

**C√≥digo Python (`env_test.py`):**

```python
import gymnasium as gym
import time

# Carregar o ambiente a partir do ficheiro XML
env = gym.make('Reacher-v4', xml_file='robot_world.xml') # Usamos um ambiente base como o Reacher para a estrutura

# Reiniciar o ambiente para obter a observa√ß√£o inicial
observation, info = env.reset()

for _ in range(1000):
    # A√ß√£o aleat√≥ria: um vetor com 3 valores entre -1 e 1 para os 3 motores
    action = env.action_space.sample()
    
    # Executar a a√ß√£o no ambiente
    observation, reward, terminated, truncated, info = env.step(action)

    # O 'observation' cont√©m o estado do mundo!
    # Para o Reacher-v4, isto inclui:
    # - cos(joint_angles), sin(joint_angles)
    # - joint_velocities
    # - end_effector_to_target_vector
    
    # Vamos extrair o nosso vetor de estado personalizado
    joint_angles = observation[:3] # Exemplo, pode precisar de ajuste
    object_pos = env.data.get_body_xpos('object')
    target_pos = env.data.get_site_xpos('target')
    
    state_vector = list(joint_angles) + list(object_pos) + list(target_pos)
    print(f"Vetor de Estado Personalizado: {len(state_vector)} dimens√µes")

    if terminated or truncated:
        observation, info = env.reset()

env.close()
```

Com isto, tem um ambiente de simula√ß√£o funcional e uma forma de extrair o vetor de estado de baixa dimens√£o necess√°rio para a Fase 1. O pr√≥ximo passo √© usar este ambiente para treinar os seus modelos.

## 2. Pipeline de Treino Detalhado (com C√≥digo PyTorch)

Agora que o ambiente est√° pronto, vamos ao treino de cada componente do World Model. Usaremos PyTorch pela sua flexibilidade.

**Instala√ß√£o:**
```bash
sudo pip3 install torch numpy cma
```

### 2.1. Treino do VAE (Componente V)

O objetivo √© aprender uma representa√ß√£o latente `z` do nosso vetor de estado.

**Passo 1: Gerar o Dataset**

Execute o seu ambiente com a√ß√µes aleat√≥rias por um longo per√≠odo (ex: 1 milh√£o de passos) e guarde cada `state_vector` num ficheiro.

```python
# generate_vae_data.py
import gymnasium as gym
import numpy as np

N_SAMPLES = 1_000_000
dataset = []

env = gym.make('Reacher-v4', xml_file='robot_world.xml')
obs, _ = env.reset()

for i in range(N_SAMPLES):
    action = env.action_space.sample()
    obs, _, terminated, truncated, _ = env.step(action)
    
    joint_angles = obs[:3] # Ajuste conforme a sua observa√ß√£o
    object_pos = env.data.get_body_xpos('object')
    target_pos = env.data.get_site_xpos('target')
    state_vector = np.concatenate([joint_angles, object_pos, target_pos])
    dataset.append(state_vector)

    if (i + 1) % 10000 == 0:
        print(f"Generated {i+1}/{N_SAMPLES} samples")
    if terminated or truncated:
        obs, _ = env.reset()

np.save('vae_dataset.npy', np.array(dataset))
env.close()
```

**Passo 2: Definir e Treinar o Modelo VAE**

```python
# train_vae.py
import torch
import torch.nn as nn
import numpy as np

STATE_DIM = 9 # 3 juntas + 3 obj_pos + 3 target_pos
LATENT_DIM = 32

class VAE(nn.Module):
    def __init__(self):
        super(VAE, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(STATE_DIM, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
        )
        self.fc_mu = nn.Linear(64, LATENT_DIM)
        self.fc_logvar = nn.Linear(64, LATENT_DIM)
        self.decoder = nn.Sequential(
            nn.Linear(LATENT_DIM, 64),
            nn.ReLU(),
            nn.Linear(64, 64),
            nn.ReLU(),
            nn.Linear(64, STATE_DIM)
        )

    def encode(self, x):
        h = self.encoder(x)
        return self.fc_mu(h), self.fc_logvar(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        return self.decoder(z)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        return self.decode(z), mu, logvar

def loss_function(recon_x, x, mu, logvar):
    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')
    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return BCE + KLD

# --- Loop de Treino ---
dataset = torch.from_numpy(np.load('vae_dataset.npy')).float()
model = VAE()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

for epoch in range(50):
    recon_batch, mu, logvar = model(dataset)
    loss = loss_function(recon_batch, dataset, mu, logvar)
    
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    print(f'Epoch {epoch}, Loss: {loss.item() / len(dataset)}')

# Salvar o modelo treinado
torch.save(model.state_dict(), 'vae.pth')
```

### 2.2. Treino do MDN-RNN (Componente M)

O objetivo √© prever o pr√≥ximo estado latente `z_{t+1}` dado o estado e a√ß√£o atuais `(z_t, a_t)`.

**Passo 1: Gerar o Dataset de S√©ries Temporais**

Similar ao VAE, mas agora guardamos sequ√™ncias de `(z, action, next_z)`.

```python
# generate_rnn_data.py
import gymnasium as gym
import numpy as np
import torch

# Carregar o VAE treinado
vae = VAE()
vae.load_state_dict(torch.load('vae.pth'))
vae.eval()

N_SEQUENCES = 10000
SEQUENCE_LEN = 100
dataset = []

env = gym.make('Reacher-v4', xml_file='robot_world.xml')

for i in range(N_SEQUENCES):
    obs, _ = env.reset()
    sequence = []
    for t in range(SEQUENCE_LEN):
        action = env.action_space.sample()
        
        # Estado atual para z_t
        state_vector = np.concatenate([obs[:3], env.data.get_body_xpos('object'), env.data.get_site_xpos('target')])
        with torch.no_grad():
            mu, _ = vae.encode(torch.from_numpy(state_vector).float())
        z = mu.numpy()

        # Step
        obs, _, terminated, truncated, _ = env.step(action)
        
        sequence.append((z, action))
        if terminated or truncated:
            break
    dataset.append(sequence)

np.save('rnn_dataset.npy', dataset, allow_pickle=True)
env.close()
```

**Passo 2: Definir e Treinar o Modelo MDN-RNN**

Este modelo √© mais complexo. Ele combina um LSTM com uma Mixture Density Network.

```python
# train_rnn.py
# (A implementa√ß√£o completa de um MDN-RNN √© extensa)
# Recomendo usar uma biblioteca como a de 'hardmaru/pytorch_mdn' no GitHub
# O conceito √©:

class MDNRNN(nn.Module):
    def __init__(self, latent_size, action_size, hidden_size, n_gaussians):
        super(MDNRNN, self).__init__()
        self.lstm = nn.LSTM(latent_size + action_size, hidden_size)
        # A cabe√ßa da MDN prev√™ os par√¢metros (pi, sigma, mu) para uma mistura de Gaussianas
        self.mdn_head = nn.Linear(hidden_size, n_gaussians * (2 * latent_size + 1))

    def forward(self, z, a, h):
        x = torch.cat([z, a], dim=1)
        out, h_next = self.lstm(x.unsqueeze(0), h)
        # ... l√≥gica para extrair pi, sigma, mu da sa√≠da da mdn_head
        return (pi, sigma, mu), h_next

# O loop de treino minimizaria a negative log-likelihood da verdadeira sequ√™ncia z_{t+1}
# sob a distribui√ß√£o de mistura prevista pelo modelo.
# torch.save(mdnrnn.state_dict(), 'mdnrnn.pth')
```

### 2.3. Treino do Controlador (Componente C)

Esta √© a parte mais "m√°gica". Usamos um otimizador de caixa-preta (CMA-ES) para treinar o controlador **dentro do sonho do World Model**.

```python
# train_controller.py
import cma
import torch

# Carregar modelos VAE e MDN-RNN
vae = VAE(); vae.load_state_dict(torch.load('vae.pth'))
mdnrnn = MDNRNN(...); mdnrnn.load_state_dict(torch.load('mdnrnn.pth'))

# Controlador √© uma rede simples
controller = nn.Linear(LATENT_DIM + HIDDEN_SIZE, ACTION_DIM)

# Fun√ß√£o de fitness: avalia um controlador num "sonho"
def evaluate_controller(weights):
    controller.load_state_dict(weights) # Carregar os pesos propostos pelo CMA-ES
    total_reward = 0

    # Iniciar um sonho
    z = torch.randn(1, LATENT_DIM)
    h = (torch.zeros(1, 1, HIDDEN_SIZE), torch.zeros(1, 1, HIDDEN_SIZE))

    for _ in range(200): # Sonhar por 200 passos
        # 1. Obter a√ß√£o do controlador
        action = controller(torch.cat([z, h[0].squeeze(0)], dim=1))
        
        # 2. Prever o pr√≥ximo estado com o RNN
        (pi, sigma, mu), h = mdnrnn(z, action, h)
        # ... l√≥gica para amostrar o pr√≥ximo z da mistura de Gaussianas
        z = sample_from_mixture(pi, sigma, mu)

        # 3. Calcular a recompensa decodificando o estado
        decoded_state = vae.decode(z)
        object_pos = decoded_state[0, 3:6]
        target_pos = decoded_state[0, 6:9]
        reward = -torch.norm(object_pos - target_pos) # Recompensa √© a dist√¢ncia negativa
        total_reward += reward

    return -total_reward # CMA-ES minimiza, ent√£o retornamos a recompensa negativa

# Otimiza√ß√£o com CMA-ES
initial_weights = controller.state_dict()
es = cma.CMAEvolutionStrategy(initial_weights_vector, 0.5)

while not es.stop():
    solutions = es.ask()
    fitness_list = [evaluate_controller(s) for s in solutions]
    es.tell(solutions, fitness_list)
    es.logger.add()
    es.disp()

# Salvar o melhor controlador encontrado
# torch.save(es.result.xbest, 'controller.pth')
```

## 3. Executando o Agente Final

Depois de treinar todos os componentes, o loop final de execu√ß√£o no ambiente real (simulado) √© surpreendentemente simples:

```python
# run_agent.py

# Carregar todos os modelos treinados (VAE, MDNRNN, Controller)

obs, _ = env.reset()
h = (torch.zeros(1, 1, HIDDEN_SIZE), torch.zeros(1, 1, HIDDEN_SIZE))

while True:
    # 1. Observar o estado real e codificar para o espa√ßo latente
    state_vector = ... # Extrair do obs
    z, _ = vae.encode(torch.from_numpy(state_vector).float())

    # 2. Obter a a√ß√£o do controlador
    action_tensor = controller(torch.cat([z, h[0].squeeze(0)], dim=1))
    action = action_tensor.detach().numpy()

    # 3. Executar a a√ß√£o no ambiente
    obs, _, _, _, _ = env.step(action)

    # 4. Atualizar o estado da mem√≥ria (opcional, mas bom para consist√™ncia)
    _, h = mdnrnn(z, action_tensor, h)
```

Este guia fornece a estrutura completa. A implementa√ß√£o de detalhes como a amostragem da MDN e a interface com o CMA-ES requerem um pouco mais de c√≥digo, mas a l√≥gica central est√° toda aqui. Comece por aqui, e ter√° um World Model funcional em pouco tempo!


## 4. Dicas Pr√°ticas e Troubleshooting

### 4.1. Acelerar o Treino

**Paraleliza√ß√£o:** Gere dados de treino em paralelo usando m√∫ltiplos processos. O MuJoCo √© thread-safe e pode correr m√∫ltiplas inst√¢ncias simultaneamente.

```python
from multiprocessing import Pool

def collect_data(seed):
    env = gym.make('Reacher-v4', xml_file='robot_world.xml')
    env.seed(seed)
    # ... coletar dados
    return data

with Pool(8) as p:
    all_data = p.map(collect_data, range(8))
```

**GPU:** Use GPU para treinar o VAE e o MDN-RNN. Mova os modelos e dados para CUDA:

```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = VAE().to(device)
dataset = dataset.to(device)
```

### 4.2. Debugging Comum

**Problema:** O VAE n√£o reconstr√≥i bem o estado.

**Solu√ß√£o:** Normalize os seus dados! Diferentes features (√¢ngulos vs posi√ß√µes) t√™m escalas diferentes.

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
dataset_normalized = scaler.fit_transform(dataset)
# Guarde o scaler para usar na infer√™ncia!
```

**Problema:** O controlador n√£o aprende nada com CMA-ES.

**Solu√ß√£o:** A fun√ß√£o de recompensa pode estar mal definida. Certifique-se de que:
- A recompensa √© densa (n√£o esparsa)
- O range de valores √© razo√°vel (ex: entre -100 e 0)
- Teste primeiro com um controlador aleat√≥rio para ver o baseline

**Problema:** O MDN-RNN prev√™ sempre o mesmo estado.

**Solu√ß√£o:** Isto √© "mode collapse". Aumente o n√∫mero de componentes Gaussianas na mistura (ex: de 5 para 10) e adicione um termo de regulariza√ß√£o para encorajar diversidade.

### 4.3. Valida√ß√£o do Sistema

Antes de considerar o treino completo, valide cada componente isoladamente:

1. **VAE:** Visualize reconstru√ß√µes. Pegue em 10 estados aleat√≥rios, passe pelo VAE, e compare o input com o output. Devem ser quase id√™nticos.

2. **MDN-RNN:** Teste a predi√ß√£o. D√™-lhe uma sequ√™ncia real `(z_t, a_t)` e veja se `z_{t+1}` previsto est√° perto do `z_{t+1}` real. Calcule o MSE.

3. **Controlador:** Execute-o no ambiente real (n√£o no sonho) e veja se consegue pelo menos aproximar-se do alvo, mesmo que de forma ineficiente.

### 4.4. Estrutura de Ficheiros Recomendada

```
project/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ vae_dataset.npy
‚îÇ   ‚îî‚îÄ‚îÄ rnn_dataset.npy
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ vae.pth
‚îÇ   ‚îú‚îÄ‚îÄ mdnrnn.pth
‚îÇ   ‚îî‚îÄ‚îÄ controller.pth
‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îî‚îÄ‚îÄ robot_world.xml
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ train_vae.py
‚îÇ   ‚îú‚îÄ‚îÄ train_rnn.py
‚îÇ   ‚îú‚îÄ‚îÄ train_controller.py
‚îÇ   ‚îî‚îÄ‚îÄ run_agent.py
‚îî‚îÄ‚îÄ requirements.txt
```

### 4.5. Pr√≥ximos Passos

Depois de ter a Fase 1 a funcionar:

1. **Adicione m√©tricas:** Taxa de sucesso, dist√¢ncia m√©dia ao alvo, suavidade da trajet√≥ria
2. **Experimente com hiperpar√¢metros:** Tamanho do espa√ßo latente, n√∫mero de componentes da MDN
3. **Prepare-se para a Fase 2:** Comece a pensar em como representar restri√ß√µes estruturadamente

Este √© o caminho. Boa sorte, e lembre-se: a primeira vers√£o n√£o ser√° perfeita. Itere, experimente, e aprenda com os erros! üöÄ
